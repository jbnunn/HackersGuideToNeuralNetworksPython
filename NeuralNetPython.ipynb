{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://karpathy.github.io/neuralnets/\n",
    "#\n",
    "# Via Hacker news:\n",
    "\n",
    "# this eventually turned into Andrej Karpathy's class at \n",
    "# Stanford, CS231n. The class notes are here: \n",
    "# http://cs231n.github.io/\n",
    "#\n",
    "# A lot of the compute graph and backprop type stuff that \n",
    "# is in the hacker's guide is covered in this specific class,\n",
    "# starting about at this time: \n",
    "# https://www.youtube.com/watch?v=i94OvYb6noo&t=207s\n",
    "\n",
    "# Note: This guide by Karpathy has been converted from JS to Python by jbnunn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "f(x,y)=xy\n",
    "\"\"\"\n",
    "def forward_multiply_gate(x, y):\n",
    "    return x*y\n",
    "\n",
    "forward_multiply_gate(2, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.950100000000001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The problem we are interested in studying looks as follows:\n",
    "\n",
    "We provide a given circuit some specific input values (e.g. x = -2, y = 3)\n",
    "The circuit computes an output value (e.g. -6)\n",
    "The core question then becomes: How should one tweak the input slightly to increase the output?\n",
    "\n",
    "In this case, in what direction should we change x,y to get a number larger than -6? \n",
    "Note that, for example, x = -1.99 and y = 2.99 gives x * y = -5.95, which is higher than -6.0. \n",
    "Don’t get confused by this: -5.95 is better (higher) than -6.0.\n",
    "It’s an improvement of 0.05, even though the magnitude of -5.95 (the distance from zero) happens to be lower.\n",
    "\"\"\"\n",
    "\n",
    "forward_multiply_gate(-1.99, 2.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best X: -1.81, Best Y: 2.99, Total: -5.4119)\n"
     ]
    }
   ],
   "source": [
    "## Strategy 1: Random Local Search\n",
    "import random\n",
    "x = -2\n",
    "y = 3\n",
    "\n",
    "# Try changing x,y randomly small amounds and keep track of what works\n",
    "tweak_amount = 0.01\n",
    "best_out = -float('inf') # infinity\n",
    "best_x = x\n",
    "best_y = y\n",
    "\n",
    "for k in range(0,100):\n",
    "    x_try = x + tweak_amount * (random.randint(0,10) * 2 - 1)\n",
    "    y_try = y + tweak_amount * (random.randint(0,10) * 2 - 1)\n",
    "    out = forward_multiply_gate(x_try, y_try)\n",
    "    \n",
    "    if out > best_out:\n",
    "        best_out = out\n",
    "        best_x = x_try\n",
    "        best_y = y_try\n",
    "\n",
    "# We should have something < -6        \n",
    "print(\"Best X: {}, Best Y: {}, Total: {})\".format(best_x, best_y, best_x * best_y))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "So, we’re done, right? Not quite: This is a perfectly fine strategy \n",
    "for tiny problems with a few gates if you can afford the compute \n",
    "time, but it won’t do if we want to eventually consider huge \n",
    "circuits with millions of inputs. \n",
    "\n",
    "It turns out that we can do much better.\n",
    "\"\"\"    \n",
    "\n",
    "## Strategy 2: Numerical Gradient\n",
    "\n",
    "\"\"\"\n",
    "Here's how to imagine this. Imagine pulling on the output value to make it \n",
    "larger. It might exert a force on X that makes the output higher, than -6 e.g.\n",
    "\"\"\"\n",
    "forward_multiply_gate(x+1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.00000000000189\n",
      "-2.0000000000042206\n",
      "x+h:  -5.9997\n",
      "y+h:  -6.0002\n",
      "-6.000000000016442\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We've just learned about the \"derivative\" of the output value with respect to \n",
    "its inputs (x and y).\n",
    "\n",
    "The derivative can be thought of as a force on each input as we pull on the \n",
    "output to become higher.\n",
    "\n",
    "It's a very simple procedure. Instead of pulling on the circuit’s output, we’ll \n",
    "iterate over every input one by one, increase it very slightly and look at what \n",
    "happens to the output value. The amount the output changes in response is the \n",
    "derivative.\n",
    "\n",
    "Here's the formula for the derivative with respect to x:\n",
    "\n",
    "∂f(x, y)     f(x + h, y) - f(x, y)\n",
    "--------  =  -------------------\n",
    "∂x                     h\n",
    "\n",
    "\n",
    "A \"derivative\" is with respect to a single input. The gradient is a collection \n",
    "of ALL the derivatives. (It's represented as a concatendated list, a vector--not shown.)\n",
    "\n",
    "\"\"\"\n",
    "x = -2\n",
    "y = 3\n",
    "h = 0.0001\n",
    "\n",
    "derivative_x = (forward_multiply_gate(x + h, y) - forward_multiply_gate(x, y)) / h\n",
    "print(derivative_x)\n",
    "\n",
    "derivative_y = (forward_multiply_gate(x, y + h) - forward_multiply_gate(x, y)) / h\n",
    "print(derivative_y)\n",
    "\n",
    "# See what happens when we turn the knob x to x + h\n",
    "x_h = forward_multiply_gate(x + h, y)\n",
    "print(\"x+h: \", x_h)\n",
    "\n",
    "# See what happens when we turn the knob y to y + h\n",
    "y_h = forward_multiply_gate(x, y + h)\n",
    "print(\"y+h: \", y_h)\n",
    "\n",
    "out = forward_multiply_gate(derivative_x, derivative_y); # \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.87059999999986\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "x = -2\n",
    "y = 3\n",
    "output = forward_multiply_gate(x, y) # Before, -6\n",
    "x = x + (step_size * derivative_x) # x becomes -1.97\n",
    "y = y + (step_size * derivative_y) # y becomes 2.98\n",
    "\n",
    "output_new = forward_multiply_gate(x, y) # -5.87, which achives the goal of being greater than the original -6\n",
    "print(output_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Strategy 3: Analytic Gradient\n",
    "\"\"\"\n",
    "In the previous section we evaluated the gradient by probing the \n",
    "circuit’s output value, independently for every input. This procedure \n",
    "gives you what we call a numerical gradient. This approach, however, \n",
    "is still expensive because we need to compute the circuit’s output as \n",
    "we tweak every input value independently a small amount. So the \n",
    "complexity of evaluating the gradient is linear in number of inputs. \n",
    "But in practice we will have hundreds, thousands or (for neural networks) \n",
    "even tens to hundreds of millions of inputs, and the circuits aren’t \n",
    "just one multiply gate but huge expressions that can be expensive to \n",
    "compute. We need something better.\n",
    "\n",
    "Luckily, there is an easier and much faster way to compute the gradient: \n",
    "we can use calculus to derive a direct expression for it that will be as\n",
    "simple to evaluate as the circuit’s output value. We call this an analytic\n",
    "gradient and there will be no need for tweaking anything.\n",
    "\n",
    "Final point: The analytic derivative requires no tweaking of the inputs. \n",
    "It can be derived using mathematics (calculus).\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
